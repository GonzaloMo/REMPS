Training:
  # (dict | callable | Stopper) – Stopping criteria. If dict, the keys may be any field in the return result of ‘train()’, 
  # whichever is reached first. If function, it must take (trial_id, result) as arguments and return a boolean 
  # (True if trial should be stopped, False otherwise). This can also be a subclass of ray.tune.Stopper, 
  # which allows users to implement custom experiment-wide stopping (i.e., stopping an entire Tune run based on some time constraint).
  stop: 
    episodes_total: 10000000


  # (str) – Local dir to save training results to. Defaults to ~/ray_results.
  local_dir: "./Logs/RL_agent/"

  # (int) – How many training iterations between checkpoints. A value of 0 (default) disables checkpointing.
  checkpoint_freq: 100

  # (int) – Seconds between global checkpointing. This does not affect checkpoint_freq, which specifies frequency for individual trials.
  # global_checkpoint_period: 1800
  # (int) – Verbosity mode. 0 = silent, 1 = only status updates, 2 = status and brief trial results, 3 = status and detailed trial results. Defaults to 3.
  verbose: 3

  # (bool) – Whether to checkpoint at the end of the experiment regardless of the checkpoint_freq. Default is False.
  checkpoint_at_end: True

  # (int) – Number of checkpoints to keep. A value of None keeps all checkpoints. Defaults to None. If set, need to provide checkpoint_score_attr.
  keep_checkpoints_num: 10

  # (str) – Specifies by which attribute to rank the best checkpoint. Default is increasing order. If attribute starts with min- it will rank attribute in decreasing order, i.e. min-validation_loss.
  # ['custom_metrics', 'episode_media', 'num_recreated_workers', 'info', 'sampler_results', 'episode_reward_max', 'episode_reward_min', 'episode_reward_mean', 'episode_len_mean', 
  # 'episodes_this_iter', 'policy_reward_min', 'policy_reward_max', 'policy_reward_mean', 'hist_stats', 'sampler_perf', 'num_faulty_episodes', 'num_healthy_workers', 'num_agent_steps_sampled', 
  # 'num_agent_steps_trained', 'num_env_steps_sampled', 'num_env_steps_trained', 'num_env_steps_sampled_this_iter', 'num_env_steps_trained_this_iter', 'timesteps_total', 
  # 'num_steps_trained_this_iter', 'agent_timesteps_total', 'timers', 'counters', 'done', 'episodes_total', 'training_iteration', 'trial_id', 'experiment_id', 'date', 'timestamp', 
  # 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'config', 'time_since_restore', 'timesteps_since_restore', 'iterations_since_restore', 'warmup_time', 'perf', 'experiment_tag']
  checkpoint_score_attr: "episode_reward_mean"

  # (list) – List of formats that exported at the end of the experiment. Default is None. Types: ['h5', ...]
  # export_formats: ['h5']

Environment:
    # main config file
    config_file: "./Configurations/SimpleWorld/planfollowing/v0.yaml"

    # (str) – The gym environment to use. This can also be a callable that returns a gym environment when called.
    env: "SimpleWorld-planfollowing-v0"

    # (func name) - The reward function to use. This can also be a callable that returns a reward function when called.
    Reward_Function: "Reward_v0"

Agent:
  # (str) – Name of the algorithm to be used. Default set to "PPO". ["PPO", "DQN", "DDPG", "TD3", "SAC"]
  Algorithm: "APPO"
  # (List[str]) – List of combinable agent Configuration files. Default ["./Configurations/Agent_Config/PPO_Config.yaml"]
  Agent_Config: ["./Configurations/RLAgent_setGoals/Agent_Config/APPO_Config.yaml"]