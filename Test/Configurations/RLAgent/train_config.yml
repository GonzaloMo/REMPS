### Trainning more properties in tune documents [https://docs.ray.io/en/releases-0.8.4/tune/api_docs/execution.html?highlight=tune.run]
Training:
  # (dict | callable | Stopper) – Stopping criteria. If dict, the keys may be any field in the return result of ‘train()’, 
  # whichever is reached first. If function, it must take (trial_id, result) as arguments and return a boolean 
  # (True if trial should be stopped, False otherwise). This can also be a subclass of ray.tune.Stopper, 
  # which allows users to implement custom experiment-wide stopping (i.e., stopping an entire Tune run based on some time constraint).
  stop: 
    timesteps_total: 10000 #100000000 

  # (str) – Local dir to save training results to. Defaults to ~/ray_results.
  local_dir: "./Logs/RLAgent_2/"

  # (int) – How many training iterations between checkpoints. A value of 0 (default) disables checkpointing.
  checkpoint_freq: 10000 #1000000

  # (bool) – Whether to checkpoint at the end of the experiment regardless of the checkpoint_freq. Default is False.
  checkpoint_at_end: True

  # (int) – Number of checkpoints to keep. A value of None keeps all checkpoints. Defaults to None. If set, need to provide checkpoint_score_attr.
  # keep_checkpoints_num: None 

  # (str) – Specifies by which attribute to rank the best checkpoint. Default is increasing order. If attribute starts with min- it will rank attribute in decreasing order, i.e. min-validation_loss.
  # ['custom_metrics', 'episode_media', 'num_recreated_workers', 'info', 'sampler_results', 'episode_reward_max', 'episode_reward_min', 'episode_reward_mean', 'episode_len_mean', 
  # 'episodes_this_iter', 'policy_reward_min', 'policy_reward_max', 'policy_reward_mean', 'hist_stats', 'sampler_perf', 'num_faulty_episodes', 'num_healthy_workers', 'num_agent_steps_sampled', 
  # 'num_agent_steps_trained', 'num_env_steps_sampled', 'num_env_steps_trained', 'num_env_steps_sampled_this_iter', 'num_env_steps_trained_this_iter', 'timesteps_total', 
  # 'num_steps_trained_this_iter', 'agent_timesteps_total', 'timers', 'counters', 'done', 'episodes_total', 'training_iteration', 'trial_id', 'experiment_id', 'date', 'timestamp', 
  # 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'config', 'time_since_restore', 'timesteps_since_restore', 'iterations_since_restore', 'warmup_time', 'perf', 'experiment_tag']
  checkpoint_score_attr: "episode_reward_mean"

  # (list) – List of formats that exported at the end of the experiment. Default is None. Types: ['h5', ...]
  export_formats: ['h5']

Environment:

  # (List[str]) – List of environments to be trainned in sequence carrying the weights. [""./Configurations/Environment_Config/Env_1.yaml""]
  Trianing_Envs: ["./Configurations/RLAgent/Environment_Config/Env_1.yaml", 
                  "./Configurations/RLAgent/Environment_Config/Env_2.yaml", 
                  "./Configurations/RLAgent/Environment_Config/Env_3.yaml", 
                  "./Configurations/RLAgent/Environment_Config/Env_4.yaml"]
  render_env: False

  env_config: 
    # (str) – The gym environment to use. This can also be a callable that returns a gym environment when called.
    env: "SimpleSatellite-setgoals-v0"

    # (str) – Module name of the Reward Function. Default set to "Reward_Function"
    Reward_Module: "Configurations.RLAgent.Reward_functions.SimpleSatSetGoals"

    # (str) – Reward function to be used. Default set to "Reward_1"
    Reward_Function: "Reward_v0"

Agent:
  # (str) – Name of the algorithm to be used. Default set to "PPO". ["PPO", "DQN", "DDPG", "TD3", "SAC"]
  Algorithm: "PPO"
  # (List[str]) – List of combinable agent Configuration files. Default ["./Configurations/Agent_Config/PPO_Config.yaml"]
  Agent_Config: ["./Configurations/RLAgent/Agent_Config/PPO_Config.yaml"]

save_dir: "./Result/RLAgent_2/"
